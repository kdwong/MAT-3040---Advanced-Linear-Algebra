\chapter{Change of Basis}
\section{Change of Basis and Matrix Representation}
\subsection{Coordinate Vector}

\begin{definition}[Coordinate Vector]\label{def:coordinate_vector}
 Let \(V\) be a finite-dimensional vector space and \(\mathcal{B} = \{ \mathbf{v}_1, \ldots, \mathbf{v}_n \}\) an ordered basis of \(V\). Any vector \(\mathbf{v} \in V\) can be uniquely written as
\[
\mathbf{v} = \alpha_1 \mathbf{v}_1 + \cdots + \alpha_n \mathbf{v}_n.
\]
We define the map \([ \cdot ]_{\mathcal{B}} : V \rightarrow \mathbb{F}^n\), which maps any vector \(\mathbf{v}\) to its coordinate vector:
\[
[ \mathbf{v} ]_{\mathcal{B}} = \begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{pmatrix}.
\]
Note that \(\{ \mathbf{v}_1, \ldots, \mathbf{v}_n \}\) and \(\{ \mathbf{v}_2, \mathbf{v}_1, \ldots, \mathbf{v}_n \}\) are distinct ordered bases.
\end{definition}

\begin{example}\label{ex:coordinate_matrix_example}
Given \(V = M_{2 \times 2}(\mathbb{F})\) and the ordered basis
\[
\mathcal{B} = \left\{ \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix} \right\},
\]
any matrix has a coordinate vector with respect to \(\mathcal{B}\), e.g.,
\[
\left[ \begin{pmatrix} 1 & 4 \\ 2 & 3 \end{pmatrix} \right]_{\mathcal{B}} = \begin{pmatrix} 1 \\ 4 \\ 2 \\ 3 \end{pmatrix}.
\]
However, if given another ordered basis
\[
\mathcal{B}_1 = \left\{ \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}, \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix} \right\},
\]
the matrix may have a different coordinate vector:
\[
\left[ \begin{pmatrix} 1 & 4 \\ 2 & 3 \end{pmatrix} \right]_{\mathcal{B}_1} = \begin{pmatrix} 4 \\ 1 \\ 2 \\ 3 \end{pmatrix}.
\]
\end{example}

\begin{theorem}\label{thm: coordinate-isomorphism}
The mapping \([ \cdot ]_{\mathcal{B}} : V \to \mathbb{F}^n\) is an isomorphism.
\end{theorem}

\begin{proof}
We prove this in several steps:

\begin{enumerate}
  \item \textbf{Well-definedness.} Let
   \(
  \mathbf{v}  = \alpha_1 \mathbf{v}_1 + \cdots + \alpha_n \mathbf{v}_n.
  \) and \(
  {\bf v'} = \alpha_1' \mathbf{v}_1 + \cdots + \alpha_n' \mathbf{v}_n.
  \)
  Suppose ${\bf v} = {\bf v'}$,  then by uniqueness of coordinates \(\alpha_i = \alpha'_i\) for all \(i = 1, \ldots, n\) and hence
  \[
  [\mathbf{v}]_{\mathcal{B}} = \begin{pmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{pmatrix}
  = \begin{pmatrix} \alpha'_1 \\ \vdots \\ \alpha'_n \end{pmatrix} = [\mathbf{v'}]_{\mathcal{B}}.
  \]
 


  \item \textbf{Linearity.} It is clear that the operator is a linear transformation:
  \[
  [p\mathbf{v} + q\mathbf{w}]_{\mathcal{B}} = p[\mathbf{v}]_{\mathcal{B}} + q[\mathbf{w}]_{\mathcal{B}} \quad \forall\, p, q \in \mathbb{F}.
  \]

  \item \textbf{Injectivity.} If
  \[
  [\mathbf{v}]_{\mathcal{B}} = \begin{pmatrix} 0 \\ \vdots \\ 0 \end{pmatrix},
  \]
  then \(\mathbf{v} = 0\mathbf{v}_1 + \cdots + 0\mathbf{v}_n = \mathbf{0}\).

  \item \textbf{Surjectivity.} Given any \(\mathbf{x} = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \in \mathbb{F}^n\), let
  \[
  \mathbf{v} := x_1 \mathbf{v}_1 + \cdots + x_n \mathbf{v}_n.
  \]
  Then \([ \mathbf{v} ]_{\mathcal{B}} = \mathbf{x}\), so every element in \(\mathbb{F}^n\) is hit.

\end{enumerate}

Therefore, \([ \cdot ]_{\mathcal{B}}\) is an isomorphism.
\end{proof}

\begin{example}\label{ex:coordinate_change}
Given a vector space \(V = P_3[x]\) and its basis \(B = \{1, x, x^2, x^3\}\).

To check if the set \(\{1 + x^2, 3 - x^3, x - x^3\}\) is linearly independent, by part (1) in \autoref{prop: isomorphism-properties} and \autoref{thm: coordinate-isomorphism}, it suffices to check whether the corresponding coordinate vectors
\[
\left\{
\begin{bmatrix}
1 \\ 0 \\ 1 \\ 0
\end{bmatrix},
\begin{bmatrix}
3 \\ 0 \\ 0 \\ -1
\end{bmatrix},
\begin{bmatrix}
0 \\ 1 \\ 0 \\ -1
\end{bmatrix}
\right\}
\]
are linearly independent, i.e., do Gaussian Elimination and check the number of pivots.
\end{example}

\begin{remark}
Here gives rise to the question: if \(\mathcal{B}_1, \mathcal{B}_2\) form two bases of \(V\), then how are \([\mathbf{v}]_{\mathcal{B}_1}, [\mathbf{v}]_{\mathcal{B}_2}\) related to each other?

Here we consider an easy example first:
\end{remark}

\begin{example}\label{ex: change-of-basis}
Consider \(V = \mathbb{R}^n\) and its basis \(\mathcal{B}_1 = \{ \mathbf{e}_1, \ldots, \mathbf{e}_n \}\). For any \(\mathbf{v} \in V\),
\[
\mathbf{v} = \begin{bmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{bmatrix}
= \alpha_1 \mathbf{e}_1 + \cdots + \alpha_n \mathbf{e}_n
\Rightarrow [\mathbf{v}]_{\mathcal{B}_1} = \begin{bmatrix} \alpha_1 \\ \vdots \\ \alpha_n \end{bmatrix}
\]

Also, we can construct a different basis \(\mathcal{B}_2\) of \(V\) as:
\[
\mathcal{B}_2 = \left\{
\begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix},
\begin{bmatrix} 1 \\ 1 \\ \vdots \\ 0 \end{bmatrix},
\ldots,
\begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}
\right\}
\]

which gives a different coordinate vector of \(\mathbf{v}\):
\[
[\mathbf{v}]_{B_2} =
\begin{bmatrix}
\alpha_1 - \alpha_2 \\
\alpha_2 - \alpha_3 \\
\vdots \\
\alpha_{n-1} - \alpha_n \\
\alpha_n
\end{bmatrix}
\]
\end{example}

\begin{proposition}[Change of Basis]\label{prop: change-of-basis}
Let \( \mathcal{A} = \{ \mathbf{v}_1, \ldots, \mathbf{v}_n \} \) and \( \mathcal{A}' = \{ \mathbf{w}_1, \ldots, \mathbf{w}_n \} \) be two ordered bases of a vector space \( V \). Define the change of basis matrix from \( \mathcal{A} \) to \( \mathcal{A}' \), say \( C_{\mathcal{A}',\mathcal{A}} := [\alpha_{ij}] \), where
\[
\mathbf{v}_j = \sum_{i=1}^n \alpha_{ij} \mathbf{w}_i.
\]
Then for any vector \( \mathbf{v} \in V \), the change of basis amounts to left-multiplying the change of basis matrix:
\begin{equation}\label{eq:change-basis}
C_{\mathcal{A}',\mathcal{A}}[\mathbf{v}]_{\mathcal{A}} = [\mathbf{v}]_{\mathcal{A}'}.
\end{equation}
Define matrix \( C_{\mathcal{A},\mathcal{A}'} := [\beta_{ij}] \), where
\[
\mathbf{w}_j = \sum_{i=1}^n \beta_{ij} \mathbf{v}_i.
\]
Then \( \left( C_{\mathcal{A},\mathcal{A}'} \right)^{-1} = C_{\mathcal{A}',\mathcal{A}} \).
\end{proposition}
\begin{proof}
\begin{enumerate}
    \item \textbf{Validity check.} Show \eqref{eq:change-basis} holds for \( \mathbf{v} = \mathbf{v}_j \), \( j = 1, \ldots, n \):
    \begin{align*}
        \text{LHS of \eqref{eq:change-basis}} &= C_{\mathcal{A}'\mathcal{A}} [{\bf v}_j]_{\mathcal{A}} =  [\alpha_{ij}] {\bf e}_j = \begin{pmatrix} \alpha_{1j} \\ \vdots \\ \alpha_{nj} \end{pmatrix}, \\
        \text{RHS of \eqref{eq:change-basis}} &= [\mathbf{v}_j]_{\mathcal{A}'} 
        = \left[ \sum_{i=1}^n \alpha_{ij} \mathbf{w}_i \right]_{\mathcal{A}'} 
        = \begin{pmatrix} \alpha_{1j} \\ \vdots \\ \alpha_{nj} \end{pmatrix}.
    \end{align*}
    So \eqref{eq:change-basis} holds for all basis vectors \( \mathbf{v}_j \).

    \item \textbf{Linearity extension.} Let \( \mathbf{v} = \sum_{j=1}^n r_j \mathbf{v}_j \). Then:

    \begin{align*}
        C_{\mathcal{A}',\mathcal{A}}[\mathbf{v}]_{\mathcal{A}} 
        &= C_{\mathcal{A}',\mathcal{A}} \left[ \sum_{j=1}^n r_j \mathbf{v}_j \right]_{\mathcal{A}}\\
        &= C_{\mathcal{A}',\mathcal{A}} \sum_{j=1}^n r_j [\mathbf{v}_j]_{\mathcal{A}} \\
        &= \sum_{j=1}^n r_j C_{\mathcal{A}',\mathcal{A}} [\mathbf{v}_j]_{\mathcal{A}}  \\
        &= \sum_{j=1}^n r_j [\mathbf{v}_j]_{\mathcal{A}'} \\
        &= \left[ \sum_{j=1}^n r_j \mathbf{v}_j \right]_{\mathcal{A}'} = [\mathbf{v}]_{\mathcal{A}'} \notag
    \end{align*}
     where the first and last line follow from the linearity of the coordinate map \( [\,\cdot\,]_{\mathcal{A}} \) and \( [\,\cdot\,]_{\mathcal{A}'} \), and the fourth equality follows from (1) above. Therefore, \eqref{eq:change-basis} is shown for all \( \mathbf{v} \in V \). 
    \item \textbf{Matrix inverse.} We verify \( C_{\mathcal{A}',\mathcal{A}} C_{\mathcal{A},\mathcal{A}'} = I_n \):

    \begin{align*}
    \mathbf{v}_j 
    &= \sum_{i=1}^n \alpha_{ij} \mathbf{w}_i 
    = \sum_{i=1}^n \alpha_{ij} \left( \sum_{k=1}^n \beta_{ki} \mathbf{v}_k \right) 
    = \sum_{i=1}^n \sum_{k=1}^n \alpha_{ij} \beta_{ki} \mathbf{v}_k \\
    &= \sum_{k=1}^n \left( \sum_{i=1}^n \beta_{ki} \alpha_{ij} \right) \mathbf{v}_k.
    \end{align*}
    
    By the uniqueness of coordinates, we imply
    \[
    \left( \sum_{i=1}^n \beta_{ki} \alpha_{ij} \right) = \delta_{jk} := 
    \begin{cases}
    1, & j = k \\
    0, & j \ne k
    \end{cases}
    \]
    
    By matrix multiplication, the \( (k,j) \)-th entry of \( C_{\mathcal{A}',\mathcal{A}} C_{\mathcal{A},\mathcal{A}'} \) is
    \[
    [C_{\mathcal{A}',\mathcal{A}} C_{\mathcal{A},\mathcal{A}'}]_{kj} = \left( \sum_{i=1}^n \beta_{ki} \alpha_{ij} \right) = \delta_{jk}
    \Rightarrow C_{\mathcal{A}',\mathcal{A}} C_{\mathcal{A},\mathcal{A}'} = I_n.
    \]

  \item \textbf{Symmetry.} Similarly,
  \begin{align*}
    \mathbf{w}_j &= \sum_{i=1}^n \beta_{ij} \mathbf{v}_i 
    = \sum_{i=1}^n \beta_{ij} \left( \sum_{k=1}^n \alpha_{ki} \mathbf{w}_k \right) 
    = \sum_{k=1}^n \left( \sum_{i=1}^n \alpha_{ki} \beta_{ij} \right) \mathbf{w}_k,
  \end{align*}
  giving \( \sum_{i=1}^n \alpha_{ki} \beta_{ij} = \delta_{kj} \), so
  \[
  C_{\mathcal{A},\mathcal{A}'} C_{\mathcal{A}',\mathcal{A}} = I_n.
  \]
\end{enumerate}
\end{proof}

\begin{example}
Back to \autoref{ex: change-of-basis}, write \( \mathcal{B}_1, \mathcal{B}_2 \) as
\[
\mathcal{B}_1 = \{ \mathbf{e}_1, \ldots, \mathbf{e}_n \}, 
\quad 
\mathcal{B}_2 = \{ \mathbf{w}_1, \ldots, \mathbf{w}_n \}
\]
and therefore \( \mathbf{w}_i = \mathbf{e}_1 + \cdots + \mathbf{e}_i \). The change of basis matrix is given by
\[
C_{\mathcal{B}_1, \mathcal{B}_2} = 
\begin{pmatrix}
1 & 1 & \cdots & 1 \\
0 & 1 & \cdots & 1 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{pmatrix}.
\]
For instance, take \( \mathbf{v} \) in the example, then one has:
\[
C_{\mathcal{B}_1, \mathcal{B}_2} [\mathbf{v}]_{\mathcal{B}_2} = 
\begin{pmatrix}
1 & 1 & \cdots & 1 \\
0 & 1 & \cdots & 1 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{pmatrix}
\begin{pmatrix}
\alpha_1 - \alpha_2 \\
\vdots \\
\alpha_{n-1} - \alpha_n \\
\alpha_n
\end{pmatrix}
=
\begin{pmatrix}
\alpha_1 \\
\vdots \\
\alpha_n
\end{pmatrix}
= [\mathbf{v}]_{\mathcal{B}_1}
\]
\end{example}

\subsection{Matrix Representations}
\begin{definition}
Let \( T : V \to W \) be a linear transformation, and
\[
\mathcal{A} = \{ \mathbf{v}_1, \ldots, \mathbf{v}_n \}, 
\quad 
\mathcal{B} = \{ \mathbf{w}_1, \ldots, \mathbf{w}_m \}
\]
be bases of \( V \) and \( W \), respectively. The matrix representation of \( T \) with respect to (w.r.t.) \( \mathcal{A} \) and \( \mathcal{B} \) is defined as \( (T)_{\mathcal{B}\mathcal{A}} := (\alpha_{ij}) \in M_{m \times n}(\mathbb{F}) \), where
\[
T(\mathbf{v}_j) = \sum_{i=1}^m \alpha_{ij} \mathbf{w}_i
\]
for $j = 1, \dots, n$.
\end{definition}


\begin{example} \label{eg:matrix_transformation}
In MAT2040, we studied \emph{matrix transformations} $T:\mathbb{F}^n \to \mathbb{F}^m$ given by ${\bf x} \mapsto A{\bf x}$ for some $A \in M_{m \times n}(\mathbb{F})$. In this section, we want to put it in the perspective of linear transformations:

    Let $A = \begin{pmatrix}
        1 & 2 \\ 3 & 4
    \end{pmatrix}$, and $T: \mathbb{R}^2 \to \mathbb{R}^2$ given by $T{\bf x} = A{\bf x}$, and $\mathcal{B} = \left\{\begin{pmatrix}
        1 \\ -1
    \end{pmatrix}, \begin{pmatrix}
        1 \\ 1
    \end{pmatrix}\right\}$, then the matrix representation of $T: \mathbb{R}^2_{\mathcal{B}} \to \mathbb{R}^2_{\mathcal{B}}$ can be calculated by:
    \begin{align*}T\begin{pmatrix}
        1 \\ -1
    \end{pmatrix} &= \begin{pmatrix}
        -1 \\ -1
    \end{pmatrix} = 0\begin{pmatrix}
        1 \\ -1
    \end{pmatrix}+ -1\begin{pmatrix}
        1 \\ 1
    \end{pmatrix}\\
    T\begin{pmatrix}
        1 \\ 1
    \end{pmatrix} &= \begin{pmatrix}
        3 \\ 7
    \end{pmatrix} = -2\begin{pmatrix}
        1 \\ -1
    \end{pmatrix}+ 5\begin{pmatrix}
        1 \\ 1
    \end{pmatrix}
    \end{align*}
Therefore,
$$T_{\mathcal{B},\mathcal{B}} = \begin{pmatrix}
        0 & -2 \\ -1 & 5
    \end{pmatrix}$$
So we end up having a different matrix than $A$!

However, if we use the usual basis $\mathcal{E} = \{{\bf e}_1, {\bf e}_2\}$, then the matrix representation of $T: \mathbb{R}^2_{\mathcal{E}} \to \mathbb{R}^2_{\mathcal{E}}$ becomes:
    \begin{align*}T({\bf e}_1) &= \begin{pmatrix}
        1 \\ 3
    \end{pmatrix} = 1\{{\bf e}_1+ 3{\bf e}_2\\
    T({\bf e}_2) &= \begin{pmatrix}
        2 \\ 4
    \end{pmatrix} = 2{\bf e}_1+ 4{\bf e}_2,
    \end{align*}
so that 
$$T_{\mathcal{E},\mathcal{E}} = \begin{pmatrix}
        1 & 2 \\ 3 & 4
    \end{pmatrix} = A$$
and one can get back our original matrix $A$. 

This generalized perspective is extremely useful in our latter studies - it allows us to understand matrix transformations using different bases (coordinate systems). In the special case when one chooses the usual basis, one can retrieve the original matrix (see \autoref{sec:similar_basis} below.
\end{example}


Here is an example of matrix representation of a non-matrix transformation:
\begin{example}
Let \( V = \mathbb{P}_3[x] \) and \( \mathcal{A} = \{ 1, x, x^2, x^3 \} \).

Let \( T : V \to V \) be defined as \( p(x) \mapsto p'(x) \) :
\[
\left\{
\begin{aligned}
T(1) &= 0 \cdot 1 + 0 \cdot x + 0 \cdot x^2 + 0 \cdot x^3 \\
T(x) &= 1 \cdot 1 + 0 \cdot x + 0 \cdot x^2 + 0 \cdot x^3 \\
T(x^2) &= 0 \cdot 1 + 2 \cdot x + 0 \cdot x^2 + 0 \cdot x^3 \\
T(x^3) &= 0 \cdot 1 + 0 \cdot x + 3 \cdot x^2 + 0 \cdot x^3
\end{aligned}
\right.
\]

We can define the change of basis matrix for the linear transformation \( T: V_{\mathcal{A}} \to V_{\mathcal{A}} \) with respect to the bases \( \mathcal{A} \) and \( \mathcal{A} \) as:
\[
T_{\mathcal{A},\mathcal{A}} =
\begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 3 \\
0 & 0 & 0 & 0
\end{pmatrix}
\]

Suppose a different basis \( \mathcal{A}' = \{ x^3, x^2, x, 1 \} \) for the output space of \( T \), i.e., \( T : V_{\mathcal{A}} \to V_{\mathcal{A}'} \). Then,
\[
T_{\mathcal{A}', \mathcal{A}} =
\begin{pmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 3 \\
0 & 0 & 2 & 0 \\
0 & 1 & 0 & 0
\end{pmatrix}
\]
for instance, since $T(x^2) = 0\cdot x^3 + 0\cdot x^2 + 2\cdot x + 0\cdot 1$ (written in the order of $\mathcal{A}'$, the third column of $T_{\mathcal{A}', \mathcal{A}}$ reads $\begin{pmatrix}
    0 \\ 0 \\ 2 \\0 
\end{pmatrix}$.
\end{example}
Observe that the coordinate vectors before and after applying \( T \) has the same matrix multiplication. For instance, if we express $T(2x^2 + 4x^3) = (4x + 12x^2)$ in terms of matrices and vectors:
\[
[2x^2 + 4x^3]_{\mathcal{A}} =
\begin{pmatrix}
0 \\ 0 \\ 2 \\ 4
\end{pmatrix},
\quad
[4x + 12x^2]_{\mathcal{A}} =
\begin{pmatrix}
0 \\ 4 \\ 12 \\ 0
\end{pmatrix},\]
Then one can easily check that the matrix representation $T_{\mathcal{A}, \mathcal{A}}$ satisfies 
$$T_{\mathcal{A}, \mathcal{A}} \cdot 
[2x^2 + 4x^3]_{\mathcal{A}}
=
[4x + 12x^2]_{\mathcal{A}} = [T(2x^2 + 4x^3)]_{\mathcal{A}}.$$
In other words, one can compute $T:V \to V$ by using $T_{\mathcal{A}, \mathcal{A}}$. More generally:
\begin{theorem}[Matrix Representation]\label{thm: matrix-Representation}
Let \( T : V \to W \) be a linear transformation of finite-dimensional vector spaces.  
Let \( \mathcal{A}, \mathcal{B} \) be the ordered bases of \( V \) and \( W \), respectively.  
Then the following diagram holds:
\begin{figure}[h!]
\centering
\[
\begin{tikzcd}[row sep=large, column sep=huge]
V \arrow[r, "T"] \arrow[d, "{[\cdot]_{\mathcal{A}}}"'] 
  & W \arrow[d, "{[\cdot]_{\mathcal{B}}}"] \\
\mathbb{F}^n \arrow[r, "T_{\mathcal{B}\mathcal{A}}"'] 
  & \mathbb{F}^m
\end{tikzcd}
\]
\end{figure}

Namely, for any \( \mathbf{v} \in V \), 
$$ T_{\mathcal{B},\mathcal{A}} [\mathbf{v}]_{\mathcal{A}} = [T (\mathbf{v})]_{\mathcal{B}}.$$
(Informally, the linear transformation $T$ `corresponds' to multiplication of coordinate matrix $T_{\mathcal{B},\mathcal{A}}$).
\end{theorem}
\begin{proof}
Suppose \( \mathcal{A} = \{ \mathbf{v}_1, \dots, \mathbf{v}_n \} \) and \( \mathcal{B} = \{ \mathbf{w}_1, \dots, \mathbf{w}_m \} \). The proof of this theorem follows the same procedure as in \autoref{prop: change-of-basis}.

\begin{enumerate}
    \item \textbf{Basis vector} We compare both sides:
\begin{align*}
\text{LHS} = T_{\mathcal{B}\mathcal{A}} [{\bf v}_j]_{\mathcal{A}} = [\alpha_{ij}] \cdot \mathbf{e}_j = \begin{pmatrix} \alpha_{1j} \\ \vdots \\ \alpha_{nj} \end{pmatrix} = \left( \sum_{i=1}^{m} \alpha_{ij} \mathbf{w}_i \right)_{\mathcal{B}} = [T (\mathbf{v}_j)]_{\mathcal{B}} = \text{RHS} 
\end{align*}
So the identity \( T_{\mathcal{B}\mathcal{A}} [\mathbf{v}_j]_{\mathcal{A}} = [T\mathbf{v}_j]_{\mathcal{B}} \) holds for all basis vectors \( \mathbf{v}_j \).

\item \textbf{Extension to arbitrary vector}
Let \( \mathbf{v} = \sum_{j=1}^n r_j \mathbf{v}_j \) with coefficients \( r_j \in \mathbb{F} \). Then:
\begin{align*}
[T]_{\mathcal{B}\mathcal{A}}([\mathbf{v}]_{\mathcal{A}}) = (T)_{\mathcal{B}\mathcal{A}}\left( \sum_{j=1}^n r_j [\mathbf{v}_j]_{\mathcal{A}} \right) 
&= \sum_{j=1}^n r_j [T]_{\mathcal{B}\mathcal{A}}[\mathbf{v}_j]_{\mathcal{A}}  \\
&= \sum_{j=1}^n r_j [T (\mathbf{v}_j)]_{\mathcal{B}}  \\
&= \left[ \sum_{j=1}^n r_j T(\mathbf{v}_j)\right]_{\mathcal{B}}\\
&= [T(\sum_{j=1}^n r_j \mathbf{v}_j)]_{\mathcal{B}} = [T( \mathbf{v})]_{\mathcal{B}}.
\end{align*}
\end{enumerate}

Thus, for all \( \mathbf{v} \in V \), we have
\[
[T]_{\mathcal{B}\mathcal{A}} [\mathbf{v}]_{\mathcal{A}} = [T(\mathbf{v})]_{\mathcal{B}},
\]
completing the proof.
\end{proof}

\begin{remark} \label{rmk:changeofbasis_matrixtrans}
Suppose 
\[ \operatorname{id}: V_{\mathcal{A}} \to V_{\mathcal{A}'}\]
is the identity transformation $\operatorname{id}({\bf v}) := {\bf v}$ for all ${\bf v} \in V$, and \( \mathcal{A}, \mathcal{A}' \) are two ordered bases of \( V \). We would like to study
the matrix representation 
$$\operatorname{id}_{\mathcal{A}',\mathcal{A}}$$
(from now on, we write $T: V_{\mathcal{A}} \to W_{\mathcal{B}}$ instead of $T:V \to W$ whenever we study the matrix representation $T_{\mathcal{B},\mathcal{A}}$). 

Firstly, \autoref{thm: matrix-Representation} implies that
\[
\operatorname{id}_{\mathcal{A}', \mathcal{A}} ([\mathbf{v}]_{\mathcal{A}}) = [\operatorname{id}(\mathbf{v})]_{\mathcal{A}'} = [\mathbf{v}]_{\mathcal{A}'}.
\]
On the other hand, \autoref{prop: change-of-basis} implies that the change of basis matrix $C_{\mathcal{A}', \mathcal{A}}$ satisfies:
\[
C_{\mathcal{A}', \mathcal{A}} ([\mathbf{v}]_{\mathcal{A}}) = [\mathbf{v}]_{\mathcal{A}'}.
\]
Therefore, 
\[ \operatorname{id}_{\mathcal{A}', \mathcal{A}} = C_{\mathcal{A}', \mathcal{A}} \] 
This shows that \autoref{thm: matrix-Representation} generalizes the change-of-basis theorem \autoref{prop: change-of-basis}.
\end{remark}

Another advantage of using matrix representation to understand linear transformation is that:
\begin{center}
\fbox{
    Composing linear transformations $S \circ T$ \quad  $\longleftrightarrow$ \quad Multiplying matrix representations $S_{\mathcal{C},\mathcal{B}} \cdot T_{\mathcal{B},\mathcal{A}}$}
\end{center}
\begin{proposition}[Functoriality] \label{prop:functoriality}
Let \( T : V \to W \), $S: W \to U$ be linear transformations, and $\mathcal{A}$, $\mathcal{B}$, $\mathcal{C}$ be ordered bases of $V$, $W$ and $U$ respectively. Then the corresponding matrix representations satisfy
\begin{equation}\label{eq:change-of-basis-matrix}
(S \circ T)_{\mathcal{C}, \mathcal{A}} = S_{\mathcal{C}, \mathcal{B}} \cdot T_{\mathcal{B}, \mathcal{A}}.
\end{equation}
\end{proposition}

\begin{proof}
Let \( \mathcal{A} = \{ \mathbf{v}_1, \ldots, \mathbf{v}_n \} \), \( \mathcal{B} = \{ \mathbf{w}_1, \ldots, \mathbf{w}_m \} \), \( \mathcal{C} = \{ \mathbf{u}_1, \ldots, \mathbf{u}_p \} \). As before, it suffices to check \eqref{eq:change-of-basis-matrix} holds on each column:
$$(S \circ T)_{\mathcal{C}, \mathcal{A}}{\bf e}_j = (S_{\mathcal{C}, \mathcal{B}} \cdot T_{\mathcal{B}, \mathcal{A}}){\bf e}_j.$$

Recall that 
\[
T\left( {\mathbf{v}}_{j}\right)  = \mathop{\sum }\limits_{i}{\left( {T}_{\mathcal{B},\mathcal{A}}\right) }_{ij}{\mathbf{w}}_{i}, \quad \quad S\left( {\mathbf{w}}_{i}\right)  = \mathop{\sum }\limits_{k}{\left( {S}_{C,\mathcal{B}}\right) }_{ki}{\mathbf{u}}_{k}
\]
Therefore,
\begin{align*}
{\left( S \circ  T\right) }_{\mathcal{C},\mathcal{A}}\mathbf{e}_{j}={\left( S \circ  T\right) }_{\mathcal{C},\mathcal{A}}{\left( {\mathbf{v}}_{j}\right) }_{\mathcal{A}} &= {\left( S \circ  T\left( {\mathbf{v}}_{j}\right) \right) }_{\mathcal{C}}\\
&= {\left\lbrack  S \circ  \left( \mathop{\sum }\limits_{i}{\left( {T}_{\mathcal{B},\mathcal{A}}\right) }_{ij}{\bf w}_{i}\right) \right\rbrack  }_{\mathcal{C}}\\
&= \mathop{\sum }\limits_{i}{\left( {T}_{\mathcal{B},\mathcal{A}}\right) }_{ij}{\left( S\left( {\bf w}_{i}\right) \right) }_{\mathcal{C}} \\
&= \mathop{\sum }\limits_{i}{\left( {T}_{\mathcal{B},\mathcal{A}}\right) }_{ij}{\left( \mathop{\sum }\limits_{k}{\left( {S}_{C,\mathcal{B}}\right) }_{ki}{\bf u}_{k}\right) }_{\mathcal{C}} \\
&= \mathop{\sum }\limits_{k}\mathop{\sum }\limits_{i}{\left( {S}_{C,\mathcal{B}}\right) }_{ki}{\left( {T}_{\mathcal{B},\mathcal{A}}\right) }_{ij}{\left( {\mathbf{u}}_{k}\right) }_{\mathcal{C}} \\
&= \mathop{\sum }\limits_{k}{\left( {S}_{\mathcal{C},\mathcal{B}}\cdot{T}_{\mathcal{B},\mathcal{A}}\right) }_{kj}{\left( {\mathbf{u}}_{k}\right) }_{C} \\
&= \mathop{\sum }\limits_{k}{\left( {S}_{C,\mathcal{B}}\cdot{T}_{\mathcal{B},\mathcal{A}}\right) }_{kj}{\mathbf{e}}_{k} \\ 
&=({S}_{C\mathcal{B}}\cdot{T}_{\mathcal{B},\mathcal{A}})\cdot {\bf e}_j
\end{align*}
Consequently, the result follows.
\end{proof}


\begin{example}
Let $\operatorname{id}:V \to V$ be the identity map, and $\mathcal{A}, \mathcal{A}'$ are two ordered bases of $V$. By \autoref{rmk:changeofbasis_matrixtrans}, $\operatorname{id}_{\mathcal{A}',\mathcal{A}} = C_{\mathcal{A}',\mathcal{A}}$ and $\operatorname{id}_{\mathcal{A},\mathcal{A}'} = C_{\mathcal{A},\mathcal{A}'}$. Now \autoref{prop:functoriality} implies that
$$(\operatorname{id} \circ \operatorname{id})_{\mathcal{A},\mathcal{A}} = \operatorname{id}_{\mathcal{A},\mathcal{A}'} \cdot \operatorname{id}_{\mathcal{A}',\mathcal{A}} = C_{\mathcal{A},\mathcal{A}'} \cdot C_{\mathcal{A}',\mathcal{A}}$$
But obviously $(\operatorname{id} \circ \operatorname{id})_{\mathcal{A},\mathcal{A}} = \operatorname{id}_{\mathcal{A},\mathcal{A}} = I_{\dim(V) \times \dim(V)}$
is the identity matrix. Therefore, we have reproved that
$$I_{\dim(V) \times \dim(V)} = C_{\mathcal{A},\mathcal{A}'} \cdot C_{\mathcal{A}',\mathcal{A}}.$$
\end{example}




\section{Relation to Similar Matrices} \label{sec:similar_basis}
{\bf We end this section with a very important observations in linear algebra:} 
Let \( T : V \to V \) be a linear operator with \( \mathcal{A}, \mathcal{A}' \) being two ordered bases of \( V \).  We wish to relate
\begin{center}
    $T_{\mathcal{A}',\mathcal{A}'}$ and $T_{\mathcal{A},\mathcal{A}}$.
\end{center}
More explicitly, rewrite $T$ by $T = \operatorname{id} \circ T \circ \operatorname{id}$. By specifying which bases we are working on, $T: V_{\mathcal{A}'} \to V_{\mathcal{A}'}$ can be understood as:
$$V_{\mathcal{A}'} \xrightarrow{\operatorname{id}} V_{\mathcal{A}} \xrightarrow{T} V_{\mathcal{A}}\xrightarrow{\operatorname{id}} V_{\mathcal{A}'}$$
By \autoref{prop:functoriality},
\[
T_{\mathcal{A}', \mathcal{A}'} = \operatorname{id}_{\mathcal{A}', \mathcal{A}} \cdot 
T_{\mathcal{A}, \mathcal{A}} \cdot \operatorname{id}_{\mathcal{A}, \mathcal{A}'}
= C_{\mathcal{A}', \mathcal{A}} \cdot T_{\mathcal{A}, \mathcal{A}} \cdot C_{\mathcal{A}, \mathcal{A}'} = (C_{\mathcal{A}, \mathcal{A}'})^{-1} T_{\mathcal{A}, \mathcal{A}} C_{\mathcal{A}, \mathcal{A}'}.
\]
Therefore, the two matrix representations \( T_{\mathcal{A}', \mathcal{A}'} \) and \( T_{\mathcal{A}, \mathcal{A}} \) are {\bf similar matrices}. In particular, they share the same characteristic polynomials, eigenvalues, determinant, etc.

\medskip
Putting this into the perspective of matrix representations, recall that we do the following in \autoref{eg:matrix_transformation}:
\begin{itemize}
    \item Let $T: \mathbb{F}^n \to \mathbb{F}^n$ be defined by $T({\bf x}) := A{\bf x}$. 
    \item Let $\mathcal{B} = \{{\bf m}_1, \dots, {\bf m}_n\}$ be an ordered basis of $\mathbb{F}^n$, then
    $$T_{\mathcal{E},\mathcal{E}} = B$$
    \item Let $\mathcal{E} = \{{\bf e}_1, \dots {\bf e}_n\}$ be the usual ordered basis of $\mathbb{F}^n$. Then one gets back the original matrix 
    $$T_{\mathcal{E},\mathcal{E}} = A$$
\end{itemize}
Let $M$ be the matrix whose columns are given by $\mathcal{B}$, then \autoref{prop: change-of-basis} implies that $C_{\mathcal{E},\mathcal{B}} = M,$
and our discussions above gives
    \[B = M^{-1}AM.\]
In particular, $A$ and $B$ are similar.

To showcase the above arguments in the setting of  \autoref{eg:matrix_transformation}, note that
$$T_{\mathcal{E},\mathcal{E}} = \begin{pmatrix}
        1 & 2 \\ 3 & 4
    \end{pmatrix} \quad \quad T_{\mathcal{B},\mathcal{B}} = \begin{pmatrix}
        0 & -2 \\ -1 & 5
    \end{pmatrix}$$
and the change of basis matrix is
$C_{\mathcal{E},\mathcal{B}} = \begin{pmatrix}
        1 & 1 \\ -1 & 1
    \end{pmatrix}$. Then one can check that
$$\begin{pmatrix}
        0 & -2 \\ -1 & 5
    \end{pmatrix} = \begin{pmatrix}
        1 & 1 \\ -1 & 1
    \end{pmatrix}^{-1}\begin{pmatrix}
        1 & 2 \\ 3 & 4
    \end{pmatrix}\begin{pmatrix}
        1 & 1 \\ -1 & 1
    \end{pmatrix}$$
Now one can understand diagonalizable matrices better: Recall in MAT2040, the following are equivalent:
\begin{itemize}
    \item $A$ is diagonalizable;
    \item There exists a basis $\mathcal{B}$ of eigenvectors;
    \item There exists an invertible $M$ such that
    $$\begin{pmatrix}
        \lambda_1 & & \\
        & \ddots & \\
        & & \lambda_n
    \end{pmatrix} = M^{-1}AM,$$
    i.e. $A$ is similar to a diagonal matrix.
\end{itemize}
Under our new perspective, we have a natural account (rather than direct computation) on why the columns of $M$ are given by the eigen-basis $\mathcal{B}$, and diagonalization is simply a change-of-basis of the {\bf same} linear transformation $T({\bf x}) = A{\bf x}$ from the usual basis $\mathcal{E}$ to the (eigen)-basis $\mathcal{B}$.

To conclude:
\begin{center}
\fbox{$A$, $B$ are {\bf similar}\quad $\longleftrightarrow$\ \quad $\begin{matrix} A, B\ \text{are matrix representations of the {\bf same} linear } \\
\text{transformation under {\bf different} coordinate systems.}  \end{matrix}$}
\end{center}


