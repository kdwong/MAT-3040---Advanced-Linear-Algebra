\chapter{Linear Transformations}
In linear algebra, we will study a special kind of functions between vector spaces:
\begin{definition}[Linear Transformation]
Let \( V \) and \( W \) be vector spaces over a field \( \mathbb{F} \). A map \( T : V \to W \) is called a \emph{linear transformation} if for all \( \alpha, \beta \in \mathbb{F} \) and all \( \mathbf{v}_1, \mathbf{v}_2 \in V \), we have
\[
T(\alpha \mathbf{v}_1 + \beta \mathbf{v}_2) = \alpha T(\mathbf{v}_1) + \beta T(\mathbf{v}_2).
\]
\end{definition}

\begin{proposition}
Let \( V, W, U \) be vector spaces and let \( T, S \) be linear transformations. Then:
\begin{enumerate}
    \item If \( S : V \to W \) and \( T : W \to U \), then the composition \( T \circ S : V \to U \) is also a linear transformation.
    \item For any linear transformation \( T : V \to W \), we have
    \[
    T(\mathbf{0}_V) = \mathbf{0}_W.
    \]
\end{enumerate}
\end{proposition}
\begin{proof} \begin{enumerate}
    \item Let \( \mathbf{v}_1, \mathbf{v}_2 \in V \) and \( \alpha, \beta \in \mathbb{F} \). Then,
    \[
    (T \circ S)(\alpha \mathbf{v}_1 + \beta \mathbf{v}_2) = T(S(\alpha \mathbf{v}_1 + \beta \mathbf{v}_2)) = T(\alpha S(\mathbf{v}_1) + \beta S(\mathbf{v}_2)) = \alpha T(S(\mathbf{v}_1)) + \beta T(S(\mathbf{v}_2)).
    \]
    Hence \( T \circ S \) is linear.

    \item Since \( T \) is linear,
    \[
    T(\mathbf{0}_V) = T(0 \cdot \mathbf{v}) = 0 \cdot T(\mathbf{v}) = \mathbf{0}_W
    \]
    for any \( \mathbf{v} \in V \).
\end{enumerate}
\end{proof}

\begin{example}
\hfill
\begin{enumerate}
    \item Let \( A \in \mathbb{R}^{m \times n} \). The map \( T : \mathbb{R}^n \to \mathbb{R}^m \) defined by
    \[
    T(\mathbf{x}) = A \mathbf{x}
    \]
    is a linear transformation.

    \item Let \( T : \mathbb{R}[x] \to \mathbb{R}[x] \) be defined by either of the following:
    \[
    T(p(x)) = \frac{d}{dx}p(x), \quad \text{or} \quad T(p(x)) = \int_0^x p(t) \, dt.
    \]
    Both are linear transformations.

    \item The map \( T : M_{n \times n}(\mathbb{R}) \to \mathbb{R} \) defined by
    \[
    T(A) = \operatorname{trace}(A) := \sum_{i=1}^n a_{ii}
    \]
    is a linear transformation.

    \item However, the map \( A \mapsto \det(A) \) is \emph{not} a linear transformation.
\end{enumerate}
\end{example}

As we saw above, many of the functions we studied in other branches of mathematics are linear transformations. One of the many advantages of linear transformation over a general function is the following:
\begin{remark} \label{rmk:linear_trans_basis}
Let $T:V \to W$ be a linear transformation, and $\mathcal{B} = \{{\bf v}_i\ |\ i \in I\}$ be a basis of $V$, then 
\begin{center}
\fbox{\bf the image of $T$ is uniquely determined by the values of $\{T({\bf v}_i)\ |\ {\bf v}_i \in \mathcal{B}\}$}.
\end{center}
Namely, since every element ${\bf v} \in V$ can be expressed as 
${\bf v} = \sum_{\ell = 1}^k \alpha_{\ell} {\bf v}_{i_{\ell}},$ then 
\begin{equation} \label{eq:linear_trans_basis}
T({\bf v}) = T\left(\sum_{\ell = 1}^k \alpha_{\ell} {\bf v}_{i_{\ell}}\right) = \sum_{\ell = 1}^k \alpha_{\ell} T\left({\bf v}_{i_{\ell}}\right).
\end{equation}
This is certainly not true for a general function - for instance, if I tell you $f: \mathbb{R} \to \mathbb{R}$ satisfies $f(1) = 1$ and $f(2) = 4$, one cannot determine $f(x)$ for all $x \in \mathbb{R}$. 

Conversely, one can
\begin{center}
\fbox{\bf Define linear transformation $T: V \to W$ by {\it just} specifying $\{T({\bf v}_i) = {\bf w}_i \ |\ {\bf v}_i \in \mathcal{B}\}$.}
\end{center}
 In such a case, the image $T({\bf v})$ for all ${\bf v} \in V$ can be obtained by extending $T$ linearly using \autoref{eq:linear_trans_basis}.
\end{remark}


\section{Kernel and Image}
\begin{definition}[Kernel and Image]
Let \( T : V \rightarrow W \) be a linear transformation.
\begin{enumerate}
    \item The \textbf{kernel} of \( T \) is
    \[
    \ker(T) = T^{-1}(\mathbf{0}) = \{ \mathbf{v} \in V \mid T(\mathbf{v}) = \mathbf{0} \}.
    \]

    \item The \textbf{image} (or range) of \( T \) is
    \[
    \operatorname{Im}(T) = T(V) =\{ T(\mathbf{v}) \in W \mid \mathbf{v} \in V \}.
    \]
\end{enumerate}
\end{definition}

\begin{example}
\hfill
\begin{enumerate}
    \item Let \( T : \mathbb{R}^n \rightarrow \mathbb{R}^n \) be a linear transformation defined by \( T(\mathbf{x}) = A\mathbf{x} \), where \( A \in \mathbb{R}^{n \times n} \). Then
    \[
    \ker(T) = \{ \mathbf{x} \in \mathbb{R}^n \mid A\mathbf{x} = \mathbf{0} \} = \operatorname{Null}(A)
    \]
    and
    \[
    \operatorname{Im}(T) = \{ A\mathbf{x} \mid \mathbf{x} \in \mathbb{R}^n \} = \operatorname{Col}(A) = \operatorname{span}\{\text{columns of } A\} \quad \text{(Column Space)}.
    \]

    \item For \( T(p(x)) = \frac{d}{dx}p(x) \), we have
    \[
    \ker(T) = \{ \text{constant polynomials} \}, \quad \operatorname{Im}(T) = \mathbb{R}[x].
    \]
\end{enumerate}
\end{example}

\begin{proposition}
The kernel and image of a linear transformation \( T : V \rightarrow W \) are vector subspaces:
\[
\ker(T) \leq V, \quad \operatorname{Im}(T) \leq W.
\]
\end{proposition}

\begin{proof}
Let \( \mathbf{v}_1, \mathbf{v}_2 \in \ker(T) \). Then for any \( \alpha, \beta \in \mathbb{F} \), we have
\[
T(\alpha \mathbf{v}_1 + \beta \mathbf{v}_2) = \alpha T(\mathbf{v}_1) + \beta T(\mathbf{v}_2) = \alpha \cdot \mathbf{0} + \beta \cdot \mathbf{0} = \mathbf{0},
\]
so \( \alpha \mathbf{v}_1 + \beta \mathbf{v}_2 \in \ker(T) \). The case for \( \operatorname{Im}(T) \) is similar.
\end{proof}

\begin{definition}
[Rank/Nullity] Let \(V, W\) be finite dimensional vector spaces over a field \(\mathbb{F}\) and \(T : V \rightarrow W\) a linear transformation. Then we define
\[
\operatorname{rank}\left( T\right) = \dim \left( \operatorname{im}\left( T\right) \right),
\]
\[
\operatorname{nullity}\left( T\right) = \dim \left( \ker \left( T\right) \right).
\]
\end{definition}

\begin{proposition}
There are alternative characterizations for the injectivity and surjectivity of a linear transformation \(T\):
\begin{enumerate}
  \item The linear transformation \(T\) is injective if and only if
  \[ \ker(T) = \{\bf 0\} \quad \Leftrightarrow \quad \operatorname{nullity}(T) = 0. \]

  \item The linear transformation \(T\) is surjective if and only if
  \[ \operatorname{im}(T) = W \quad \Leftrightarrow \quad \operatorname{rank}(T) = \dim(W). \]

  \item If \(T\) is bijective, then \(T^{-1}\) is a linear transformation.
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}
  \item[(1)]
  \begin{enumerate}
    \item For the forward direction:
    \[
    \mathbf{x} \in \ker(T) \Rightarrow T(\mathbf{x}) = 0 = T(\mathbf{0}) \Rightarrow \mathbf{x} = \mathbf{0}.
    \]
    \item For the reverse direction:
    \[
    T(\mathbf{x}) = T(\mathbf{y}) \Rightarrow T(\mathbf{x} - \mathbf{y}) = 0 \Rightarrow \mathbf{x} - \mathbf{y} \in \ker(T) = \{0\} \Rightarrow \mathbf{x} = \mathbf{y}.
    \]
  \end{enumerate}

  \item[(2)] The proof follows a similar idea as in (1).

  \item[(3)] Let \(T^{-1} : W \rightarrow V\). For all \(\mathbf{w}_1, \mathbf{w}_2 \in W\), there exist \(\mathbf{v}_1, \mathbf{v}_2 \in V\) such that \(T(\mathbf{v}_i) = \mathbf{w}_i\), i.e.,
  \[ T^{-1}(\mathbf{w}_i) = \mathbf{v}_i, \quad i = 1, 2. \]
  Consider the mapping:
  \[
  T(\alpha \mathbf{v}_1 + \beta \mathbf{v}_2) = \alpha T(\mathbf{v}_1) + \beta T(\mathbf{v}_2) = \alpha \mathbf{w}_1 + \beta \mathbf{w}_2,
  \]
  which implies
  \[
  \alpha \mathbf{v}_1 + \beta \mathbf{v}_2 = T^{-1}(\alpha \mathbf{w}_1 + \beta \mathbf{w}_2),
  \]
  i.e.,
  \[
  \alpha T^{-1}(\mathbf{w}_1) + \beta T^{-1}(\mathbf{w}_2) = T^{-1}(\alpha \mathbf{w}_1 + \beta \mathbf{w}_2).
  \]
\end{enumerate}
\end{proof}


\section{Space of Linear Transformations}
\begin{definition}[Hom space]
Let
\[ {\operatorname{Hom}}_{\mathbb{F}}\left( V, W\right) = \{ \text{all linear transformations } T : V \rightarrow W \}, \]
and we can define the addition and scalar multiplication to make it a vector space:

\begin{enumerate}
  \item For \(T, S \in \operatorname{Hom}_{\mathbb{F}}\left( V, W\right)\), define
  \[
  (T + S)(\mathbf{v}) := T(\mathbf{v}) + S(\mathbf{v}),
  \]
  one can check easily that $(T + S)(\alpha{\bf v} + \beta{\bf w}) = \alpha(T + S)({\bf v}) + \beta(T + S)({\bf w})$. Therefore, \(T + S \in \operatorname{Hom}_{\mathbb{F}}\left( V, W\right)\).

  \item Also, define
  \[
  (\gamma T)(\mathbf{v}) := \gamma T(\mathbf{v}), \quad \text{for all } \gamma \in \mathbb{F},
  \]
  as before, one can easily check that \(\gamma T \in \operatorname{Hom}_{\mathbb{F}}\left( V, W\right)\).
\end{enumerate}
Therefore, $T+S$, $\gamma T$ are the addition and scalar multiplication operations on the vector space $\operatorname{Hom}_{\mathbb{F}}\left( V, W\right)$.

\medskip   
In particular, if \(V = \mathbb{F}^n, W = \mathbb{F}^{m}\), then by MAT2040, all linear transformations between $V$ and $W$ are just matrix multiplications, i.e.:
\[ \operatorname{Hom}_{\mathbb{F}}\left( V, W\right) = \{T:V \to W\ |\ T({\bf v}) = A{\bf v} \text{ for some } A\in M_{m \times n}(\mathbb{F})\}. \]
So 
$$\operatorname{Hom}_{\mathbb{F}}\left( V, W\right)\xleftrightarrow{1:1} M_{m \times n}(\mathbb{F}).$$ 
In fact, the relationship between them is more than just a bijection - as we will see later, it is an {\bf isomorphism} (\autoref{def:isomorphism}) between vector spaces!
\end{definition}

\begin{proposition}
If \(\dim(V) = n, \dim(W) = m\), then \(\dim\left( \operatorname{Hom}_{\mathbb{F}}(V, W) \right) = mn\).
\end{proposition}



\section{Isomorphism of Vector Spaces}
\begin{definition} \label{def:isomorphism}
[Isomorphism] We say that the vector subspaces 
\[V \cong W\]
are {\bf isomorphic} if there exists a bijective linear transformation \(T : V \rightarrow W\). This mapping \(T\) is called an \emph{isomorphism} from \(V\) to \(W\).
\end{definition}

\begin{example} \label{eg:isodim}
Let \(\dim(V) = n\) and  \(\dim(W) =m \) with \( n,m < \infty\). Then $n = m$ implies \(V \cong W\). More precisely, suppose \(\{ \mathbf{v}_1, \ldots, \mathbf{v}_n \}, \{ \mathbf{w}_1, \ldots, \mathbf{w}_n \}\) are bases of \(V\) and \(W\) respectively, define \(T : V \rightarrow W\) by:
\[
T(\alpha_1 \mathbf{v}_1 + \cdots + \alpha_n \mathbf{v}_n) = \alpha_1 \mathbf{w}_1 + \cdots + \alpha_n \mathbf{w}_n, \quad \forall \alpha_i \in \mathbb{F}.
\]
In particular, \(T(\mathbf{v}_i) = \mathbf{w}_i\) for all \(i\), and it is clear that our constructed \(T\) is a bijective linear transformation.

Note that the converse also holds, i.e. if $V \cong W$ then $n = m$. This is given by the proposition below.
\end{example}


\begin{proposition}\label{prop: isomorphism-properties}
If \(T : V \rightarrow W\) is an isomorphism, then:
\begin{enumerate}
  \item The set \(\{ \mathbf{v}_1, \ldots, \mathbf{v}_k \}\) is linearly independent in \(V\) if and only if \(\{ T\mathbf{v}_1, \ldots, T\mathbf{v}_k \}\) is linearly independent in \(W\).
  Note the same holds if we replace ``linearly independent'' with ``spans.''
  \item If \(\dim(V) = n\), then \(\{ \mathbf{v}_1, \ldots, \mathbf{v}_n \}\) forms a basis of \(V\) if and only if \(\{ T\mathbf{v}_1, \ldots, T\mathbf{v}_n \}\) forms a basis of \(W\). In particular, \(\dim(V) = \dim(W)\).
  \item Two vector spaces with finite dimensions are isomorphic if and only if they have the same dimension.
\end{enumerate}
\end{proposition}

\begin{proof}
Exercise. %It suffices to show the reverse direction. Let \(\{ \mathbf{v}_1, \ldots, \mathbf{v}_n \}\) and \(\{ \mathbf{w}_1, \ldots, \mathbf{w}_n \}\) be bases of \(V\) and \(W\), respectively. Define the linear transformation \(T : V \rightarrow W\) by
%\[
%T(a_1 \mathbf{v}_1 + \cdots + a_n \mathbf{v}_n) = a_1 \mathbf{w}_1 + \cdots + a_n \mathbf{w}_n.
%\]
%Then \(T\) is surjective since \(\{ \mathbf{w}_1, \ldots, \mathbf{w}_n \}\) spans \(W\), and injective since \(\{ \mathbf{w}_1, \ldots, \mathbf{w}_n \}\) is linearly independent.
\end{proof}


 

\begin{remark}
Note that \(V \cong W\) does not imply that any linear transformation \(T : V \rightarrow W\) is an isomorphism. For example, \(T(\mathbf{v}) = \mathbf{0}\) is not an isomorphism if \(W \neq \{ \mathbf{0} \}\).
\end{remark}


\begin{theorem}[Rank–Nullity Theorem]
Let \(T : V \rightarrow W\) be a linear transformation with \(\dim(V) < \infty\). Then
\[
\operatorname{rank}(T) + \operatorname{nullity}(T) = \dim(V).
\]
\end{theorem}

\begin{proof}
Since \(\ker(T) \leq V\), by \autoref{prop:complementation}, there exists a subspace \(U \leq V\) such that
\[
V = \ker(T) \oplus U.
\]
\begin{enumerate}
  \item Consider the restricted map \(T|_U : U \rightarrow T(U)\). This is an isomorphism because:
  \begin{itemize}
    \item It is surjective by definition of the codomain \(T(U)\).
    \item If \({\bf v} \in \ker(T|_U)\), then \(T({\bf v}) = 0\), so \({\bf v} \in \ker(T)\). But \({\bf v} \in U\) as well, and since \(U \cap \ker(T) = \{{\bf 0}\}\), we conclude \({\bf v} = {\bf 0}\).
  \end{itemize}
  Therefore, \(\dim(U) = \dim(T(U))\) by the end of \autoref{eg:isodim}.
  
  \item Note that \(\operatorname{im}(T) = T(V) = T(U)\), because for any \(v \in V\), we can write \({\bf v} = {\bf v}_0 + {\bf u}\) with \({\bf v}_0 \in \ker(T), {\bf u} \in U\), and then
  \[
  T({\bf v}) = T({\bf v}_0 + {\bf u}) = T({\bf v}_0) + T({\bf u}) = {\bf 0} + T({\bf u}) = T({\bf u}).
  \]
  Hence \(T(V) \subseteq T(U)\). But obviously one has $U \subseteq V \Rightarrow T(U) \subseteq T(V)$ as well. So $T(V) = T(U)$.
  
  \item Since \(V = \ker(T) \oplus U\), we have:
  \[
  \dim(V) = \dim(\ker(T)) + \dim(U).
  \]
  by the proof of \autoref{prop:complementation}.
\end{enumerate}

Therefore,
\[
\dim(V) = \operatorname{nullity}(T) + \dim(T(U)) = \operatorname{nullity}(T) + \dim(T(V)) = \operatorname{nullity}(T) + \operatorname{rank}(T). \qedhere
\]
\end{proof}